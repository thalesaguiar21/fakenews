{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Recorrentes\n",
    "\n",
    "Long-Short Term Memory (LSTM) são um tipo de rede neural, mais especificamente um tipo de rede neural recorrente.\n",
    "\n",
    "Redes neurais recorrentes possuem uma característica distinta de redes neurais feedforward. Ao invés da informação seguir um fluxo contínuo sempre em um direção (usualmente para \"frente\"), as RNNs passam a informação também de volta (\"trás\"). Isso permite que essas simulem uma memória, sendo capazes de lidar melhor com problemas que variam com o tempo, como é o nosso caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from context import fakenews\n",
    "from fakenews import preprocess as pre\n",
    "import gensim\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos carregador os dados que haviamos processado em aulas anteriores. Não usaremos eles completamente, mas vamos usar o modelo gerado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully read data from:\n",
      "Fakes: /home/thales/dev/fakenews/data/Fake.csv\n",
      "Reals: /home/thales/dev/fakenews/data/True.csv\n",
      "Removing rows without text...\n",
      "Removing publisher information...\n",
      "Adding class column...\n",
      "Merging fakes and reals\n",
      "Merging titles and bodies...\n",
      "Removing subjects and date...\n",
      "Tokenizing data...\n",
      "Truncating at 869\n"
     ]
    }
   ],
   "source": [
    "base = '/home/thales/dev/fakenews/'\n",
    "news, labels = pre.run(base + 'data/Fake.csv', base + 'data/True.csv')\n",
    "pre.truncate_news(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a grande quantidade de dados da base, ficaria ruim pra executar os experimentos durante a aula e mesmo para entender como funciona a rede. Portanto, vamos definir um pequeno conjunto de 3 frases para treinar a rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = ['trump may leave the white house next year',\n",
    "              'corona virus causes respiratory issues',\n",
    "              'hospitals have no patients infected with new virus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, carregamos o modelo w2v que geramos nas aulas anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load('fakenews-w2v.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, criamos uma função simples para auxiliar na conversão das features para `ndarrays`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2vec(seqs):\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        vec_seq = []\n",
    "        for word in seq.split(' '):\n",
    "            if word in w2v.wv:\n",
    "                vec_seq.append(w2v.wv[word])\n",
    "        data.append(np.array(vec_seq).copy())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar como ficam as amostras do `small_data` após a conversão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8907309e-01, -6.1274016e-01,  1.1100110e+00, -2.4788096e+00],\n",
       "       [ 1.6640684e-02,  6.2638223e-02, -1.6004701e+00,  8.4372061e-01],\n",
       "       [ 9.7744155e-04,  1.3789764e-01, -5.0027990e-01,  1.1279483e+00],\n",
       "       [ 1.9337800e+00, -4.2167101e+00,  2.6433957e+00,  2.3664207e+00],\n",
       "       [ 2.7858514e-01,  2.8959017e+00,  2.6175920e-02,  4.3230334e-01],\n",
       "       [ 7.6738346e-01, -2.8276157e+00,  7.1034896e-01,  1.7026479e+00],\n",
       "       [-2.0402887e+00, -4.7774467e-01, -2.8270140e+00,  1.5130302e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2vec(small_data)[0][:, :4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguindo essa linha de pensamento, abaixo fazemos a conversão dos dados para tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8907e-01, -6.1274e-01,  1.1100e+00, -2.4788e+00],\n",
      "        [ 1.6641e-02,  6.2638e-02, -1.6005e+00,  8.4372e-01],\n",
      "        [ 9.7744e-04,  1.3790e-01, -5.0028e-01,  1.1279e+00],\n",
      "        [ 1.9338e+00, -4.2167e+00,  2.6434e+00,  2.3664e+00],\n",
      "        [ 2.7859e-01,  2.8959e+00,  2.6176e-02,  4.3230e-01],\n",
      "        [ 7.6738e-01, -2.8276e+00,  7.1035e-01,  1.7026e+00],\n",
      "        [-2.0403e+00, -4.7774e-01, -2.8270e+00,  1.5130e+00]])\n"
     ]
    }
   ],
   "source": [
    "vec_small_data = seq2vec(small_data)\n",
    "small_tensor_data = [torch.from_numpy(vec) for vec in vec_small_data]\n",
    "print(small_tensor_data[0][:, :4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman RNN\n",
    "\n",
    "Quanto a rede recorrente em si, vamos implementar uma rede de Elman (também conhecida como rede recorrentes simples). O comportamento do modelo pode ser descrito pelas duas equações a seguir.\n",
    "\n",
    "$$h_{t} = \\sigma_{h}(W_{h}x_{t} + U_{h}h_{t-1} + b_{h})$$\n",
    "$$y_{t} = \\sigma_{y}(W_{y}h_{t} + b_{y})$$\n",
    "\n",
    "Note que podemos representar $W_{h}x_{t} + U_{h}h_{t-1}$ como $P_{h}X$, onde $P_{h} = [W_{h}\\quad U_{h}]$ e $X = [x_{t}\\quad h_{t-1}]$. Logo, podemos reescrever $h_{t}$ como\n",
    "\n",
    "$$h_{t} = \\sigma_{h}(P_{h}X + b_{h})$$\n",
    "\n",
    "Ou seja, temos duas regressões não lineare! Vamos implementar a RNN usando a classe `Module` do PyTroch. Note que o framework possui sua própria implementaçao de células RNN e LSTM (dentre vários outros tipos de layers). Entretanto, é bastante trabalho converter os dados para ajustar a entrada esperada. Além disso, existe apenas 1 tipo de célula implementada. Obviamente, a célula do framework possui vantagem com relação a performance, além de permitir a implementação de `stacks` de forma mais simples.\n",
    "\n",
    "Abaixo, definimos uma rede com uma camada linear, responsável por calcular o $h_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Linear(din + dh, dout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note a dimensao de entrada, a qual é a soma da dimensão oculta com a de entrada. Isto porque estamos usando a nossa representação $P_{h}$. A seguir, vamos adicionar nosso segundo layer, referente ao $y_{t}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Linear(din + dh, dh)\n",
    "        self.hidden = torch.nn.Linear(dh, dout)\n",
    "        self.func = torch.sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de definir a camada do $y_{t}$, também aproveitamos para definir a função de ativação. Aqui vamos usar uma sigmoid pois nosso problema é uma classificação binária (verificação). Caso fosse um problema multi-classe, a dimensão `dout` teria um valor acima de 1, e nesse caso outras funções poderiam  ser usadas. Por exemplo o softmax, ou mesmo adicionar outra camada para combinar os resultados.\n",
    "\n",
    "Agora, precisamos definir o comportamento da rede durante o feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.embeeding = w2v\n",
    "        self.input = torch.nn.Linear(din + dh, dh)\n",
    "        self.hidden = torch.nn.Linear(dh, dout)\n",
    "        self.func = torch.sigmoid\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        xt_ht = torch.cat((X, hidden), 0)\n",
    "        hidden = self.func(self.input(xt_ht))\n",
    "        output = self.hidden(hidden)\n",
    "        return self.func(output), hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembrando que o PyTroch usa o autograd para gravar as operações realizadas nos tensores e calcular o gradiente automaticamente. A função `torch.cat()` gera o $P_{h}$, ou seja, ela concatena dois tensores em uma dada dimensão (`0` no código acima). E é isso, temos nossa arquitetura pronta! Só precisamos definir a função objetivo (loss) e o seu treinamento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optei por usar o MSE, mas lembrando que poderia ser outra. Existem várias disponíveis no módulo `torch.nn.functional` assim como outras definidas fora dele: `torch.sigmoid` ou `torch.nn.CrossEntropy()`.\n",
    "\n",
    "Finalmente, vamos definir como o modelo é treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, optim, nepochs):\n",
    "    for epoch in range(nepochs):\n",
    "        optim.zero_grad()\n",
    "        outputs = []\n",
    "        for new, lbl in zip(X, y):\n",
    "            hidden = torch.zeros(50)\n",
    "            for word in new:\n",
    "                output, hidden = model(word, hidden)\n",
    "            loss = criterion(output, lbl)\n",
    "            loss.backward()\n",
    "            print(f\"{epoch} Loss = {loss}\")\n",
    "            optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provavelmente aqui é onde ocorre a principal diferençça entre as redes não-recorrentes. Veja que damos como entrada uma palavra por vez, e usamos apenas o a ultima saída como resultado final da classificação! Essa saída (output) é a que foi obtida levando em considração todo o contexto (frase). Com isso a parte, o treinamento é bem padrão para os modelos do PyTroch! Vejam que é bem parecido com o que usados nas redes neurais da aula passada.\n",
    "\n",
    "Agora, só precisamos instanciar a rede, o otimizador e definir os rótulos de cada frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss = 0.1570308655500412\n",
      "0 Loss = 0.32378053665161133\n",
      "0 Loss = 0.1612144410610199\n",
      "1 Loss = 0.1484067291021347\n",
      "1 Loss = 0.3238369822502136\n",
      "1 Loss = 0.15681371092796326\n",
      "2 Loss = 0.14053837954998016\n",
      "2 Loss = 0.3234764635562897\n",
      "2 Loss = 0.15282148122787476\n",
      "3 Loss = 0.13335317373275757\n",
      "3 Loss = 0.32272768020629883\n",
      "3 Loss = 0.14919863641262054\n",
      "4 Loss = 0.12678398191928864\n",
      "4 Loss = 0.32161903381347656\n",
      "4 Loss = 0.14590942859649658\n",
      "5 Loss = 0.12076929956674576\n",
      "5 Loss = 0.32017868757247925\n",
      "5 Loss = 0.14292128384113312\n",
      "6 Loss = 0.11525321751832962\n",
      "6 Loss = 0.31843388080596924\n",
      "6 Loss = 0.14020460844039917\n",
      "7 Loss = 0.11018522828817368\n",
      "7 Loss = 0.3164110779762268\n",
      "7 Loss = 0.13773268461227417\n",
      "8 Loss = 0.10552000254392624\n",
      "8 Loss = 0.31413528323173523\n",
      "8 Loss = 0.1354813277721405\n",
      "9 Loss = 0.10121671855449677\n",
      "9 Loss = 0.3116307556629181\n",
      "9 Loss = 0.13342860341072083\n"
     ]
    }
   ],
   "source": [
    "elman_rnn = ElmanRNN(100, 50, 1)\n",
    "sgd = torch.optim.SGD(elman_rnn.parameters(), lr=5e-3)\n",
    "y = torch.tensor([0., 1., 0.]).view(3, -1)\n",
    "train(elman_rnn, small_tensor_data, y, sgd, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
