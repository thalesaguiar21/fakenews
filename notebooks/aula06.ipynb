{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neurais Recorrentes\n",
    "\n",
    "Long-Short Term Memory (LSTM) são um tipo de rede neural, mais especificamente um tipo de rede neural recorrente.\n",
    "\n",
    "Redes neurais recorrentes possuem uma característica distinta de redes neurais feedforward. Ao invés da informação seguir um fluxo contínuo sempre em um direção (usualmente para \"frente\"), as RNNs passam a informação também de volta (\"trás\"). Isso permite que essas simulem uma memória, sendo capazes de lidar melhor com problemas que variam com o tempo, como é o nosso caso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from context import fakenews\n",
    "from fakenews import preprocess as pre\n",
    "import gensim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully read data from:\n",
      "Fakes: /home/thales/dev/fakenews/data/Fake.csv\n",
      "Reals: /home/thales/dev/fakenews/data/True.csv\n",
      "Removing rows without text...\n",
      "Removing publisher information...\n",
      "Adding class column...\n",
      "Merging fakes and reals\n",
      "Merging titles and bodies...\n",
      "Removing subjects and date...\n",
      "Tokenizing data...\n",
      "Truncating at 869\n"
     ]
    }
   ],
   "source": [
    "base = '/home/thales/dev/fakenews/'\n",
    "news, labels = pre.run(base + 'data/Fake.csv', base + 'data/True.csv')\n",
    "pre.truncate_news(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_data = ['trump may leave the white house next year',\n",
    "              'corona virus causes respiratory issues',\n",
    "              'hospitals have no patients infected with new virus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec.load('fakenews-w2v.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2vec(seqs):\n",
    "    data = []\n",
    "    for seq in seqs:\n",
    "        vec_seq = []\n",
    "        for word in seq.split(' '):\n",
    "            if word in w2v.wv:\n",
    "                vec_seq.append(w2v.wv[word])\n",
    "        data.append(np.array(vec_seq).copy())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8907309e-01, -6.1274016e-01,  1.1100110e+00, -2.4788096e+00],\n",
       "       [ 1.6640684e-02,  6.2638223e-02, -1.6004701e+00,  8.4372061e-01],\n",
       "       [ 9.7744155e-04,  1.3789764e-01, -5.0027990e-01,  1.1279483e+00],\n",
       "       [ 1.9337800e+00, -4.2167101e+00,  2.6433957e+00,  2.3664207e+00],\n",
       "       [ 2.7858514e-01,  2.8959017e+00,  2.6175920e-02,  4.3230334e-01],\n",
       "       [ 7.6738346e-01, -2.8276157e+00,  7.1034896e-01,  1.7026479e+00],\n",
       "       [-2.0402887e+00, -4.7774467e-01, -2.8270140e+00,  1.5130302e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2vec(small_data)[0][:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8907e-01, -6.1274e-01,  1.1100e+00, -2.4788e+00],\n",
      "        [ 1.6641e-02,  6.2638e-02, -1.6005e+00,  8.4372e-01],\n",
      "        [ 9.7744e-04,  1.3790e-01, -5.0028e-01,  1.1279e+00],\n",
      "        [ 1.9338e+00, -4.2167e+00,  2.6434e+00,  2.3664e+00],\n",
      "        [ 2.7859e-01,  2.8959e+00,  2.6176e-02,  4.3230e-01],\n",
      "        [ 7.6738e-01, -2.8276e+00,  7.1035e-01,  1.7026e+00],\n",
      "        [-2.0403e+00, -4.7774e-01, -2.8270e+00,  1.5130e+00]])\n"
     ]
    }
   ],
   "source": [
    "vec_small_data = seq2vec(small_data)\n",
    "small_tensor_data = [torch.from_numpy(vec) for vec in vec_small_data]\n",
    "print(small_tensor_data[0][:, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Linear(din + dh, dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.input = torch.nn.Linear(din + dh, dh)\n",
    "        self.hidden = torch.nn.Linear(dh, dout)\n",
    "        self.func = torch.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmanRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dh, dout):\n",
    "        super().__init__()\n",
    "        self.embeeding = w2v\n",
    "        self.input = torch.nn.Linear(din + dh, dh)\n",
    "        self.hidden = torch.nn.Linear(dh, dout)\n",
    "        self.func = torch.sigmoid\n",
    "        \n",
    "    def forward(self, X, hidden):\n",
    "        xt_ht = torch.cat((X, hidden), 0)\n",
    "        hidden = self.func(self.input(xt_ht))\n",
    "        output = self.hidden(hidden)\n",
    "        return self.func(output), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.functional.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, optim, nepochs):\n",
    "    for epoch in range(nepochs):\n",
    "        optim.zero_grad()\n",
    "        outputs = []\n",
    "        for new, lbl in zip(X, y):\n",
    "            hidden = torch.zeros(50)\n",
    "            for word in new:\n",
    "                output, hidden = model(word, hidden)\n",
    "            loss = criterion(output, lbl)\n",
    "            loss.backward()\n",
    "            print(f\"{epoch} Loss = {loss}\")\n",
    "            optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Loss = 0.1570308655500412\n",
      "0 Loss = 0.32378053665161133\n",
      "0 Loss = 0.1612144410610199\n",
      "1 Loss = 0.1484067291021347\n",
      "1 Loss = 0.3238369822502136\n",
      "1 Loss = 0.15681371092796326\n",
      "2 Loss = 0.14053837954998016\n",
      "2 Loss = 0.3234764635562897\n",
      "2 Loss = 0.15282148122787476\n",
      "3 Loss = 0.13335317373275757\n",
      "3 Loss = 0.32272768020629883\n",
      "3 Loss = 0.14919863641262054\n",
      "4 Loss = 0.12678398191928864\n",
      "4 Loss = 0.32161903381347656\n",
      "4 Loss = 0.14590942859649658\n",
      "5 Loss = 0.12076929956674576\n",
      "5 Loss = 0.32017868757247925\n",
      "5 Loss = 0.14292128384113312\n",
      "6 Loss = 0.11525321751832962\n",
      "6 Loss = 0.31843388080596924\n",
      "6 Loss = 0.14020460844039917\n",
      "7 Loss = 0.11018522828817368\n",
      "7 Loss = 0.3164110779762268\n",
      "7 Loss = 0.13773268461227417\n",
      "8 Loss = 0.10552000254392624\n",
      "8 Loss = 0.31413528323173523\n",
      "8 Loss = 0.1354813277721405\n",
      "9 Loss = 0.10121671855449677\n",
      "9 Loss = 0.3116307556629181\n",
      "9 Loss = 0.13342860341072083\n"
     ]
    }
   ],
   "source": [
    "elman_rnn = ElmanRNN(100, 50, 1)\n",
    "sgd = torch.optim.SGD(elman_rnn.parameters(), lr=5e-3)\n",
    "y = torch.tensor([0., 1., 0.]).view(3, -1)\n",
    "train(elman_rnn, small_tensor_data, y, sgd, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treina 1 em 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enquanto que para usar todos os elementos precisamos concatenar nossos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todos de uma vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principalmente em NLP, a transformação do texto para representação numérica é bastante comum. Dessa forma, podemos criar uma LSTM customizada ao adicionar um layer de embedding que fica responsável por essa transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMfnews:\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como podemos usar o novo layer durante o treinamento, aprendendo os embeddings de acordo com a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treina embedding + lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que também podemos criar vários franksteins, da mesma forma que fizemos com a RNN. Note que o *gradient descent* é totalmente genérico. Isto é, ele não depende da arquitetura do seu modelo, apenas que as funções de ativação sejam deriváveis ou deriváveis por partes. Por isso podemos criar estruturas totalmente **idiotas**, com valores indo, voltando, pulando, ignorando, com muito layer, e com o que quisermos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cria frankstein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É importante notar que redes profundas tendem a ocasionar *overfitting*, assim como os problemas decorrentes do algoritmo de aprendizado:\n",
    "\n",
    "* Vanishing\n",
    "* Exploding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
